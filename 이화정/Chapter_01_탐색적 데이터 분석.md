# [ CHAPTER 1 ] 탐색적 데이터 분석

## 1.1 정형화된 데이터의 요소

- 고전적 통계학 : 추론 (적은 표본을 가지고 모집단을 도출)하는 것에 다뤘다.
- 최근에는 데이터 분석이라는 새로운 과학적 학문을 제시했다.
1. 데이터의 종류
    1. 수치형 데이터 : 숫자를 이용해 표현할 수 있는 데이터
        1. 연속형 데이터 : 일정 범위 안에서 어떤 값이든 취할 수 있는 데이터

            == 구간형, 실수형, 수치형데이터

            Ex, 풍속, 지속시간

        2. 이산데이터 : 횟수와 같은 정수값만 취할수있는 데이터

            == 정수형, 횟수 데이터

            Ex, 사건의 발생빈도

    2. 범주형데이터 : 가능한 범주 안의 값만을 취하는 데이터

        == 목록, 열거 요인, 명목, 다항형데이터 도시명 - 범위가 정해진 값들

        1. 이진데이터 : 범주형 데이터 중에서도 두 값중 하나를 갖는 아주 특수한 경우
            1. 두개의 값만을 갖는 범주형데이터의 특수한 경우

            == 이항적, 논리형, 지표, 분리언 데이터

        2. 순서형 데이터 : 범주안의 값들이 순위를 갖는 데이터

            : 값들 사이에 분명한 순위가 있는 범주형 데이터

            == 정렬된 요인 데이터

            ex, 평점

데이터 종류를 분류하는 일은 데이터 분석하고 예측, 모델링 등에 중요한 역할을 한다.
소프트 웨어에서 데이터 종류를 정하는 것은 해당 데이터를 어떻게 처리할지를 정하는 것과 같다.

## 1.2. 테이블 데이터

: 데이터 분석에서 가장 대표적으로 사용되는 객체 형태는 테이블 데이터이다.

: 테이블 데이터는 엑셀이나 데이터 베이스의 테이블 같은 형태를 의미한다.

: 각 레코드를 나타내는 행과 피처를 나타내는 열로 이루어진 이차원 행렬을 의미함.

- 용어
    - 피처 feature : 일반적으로 테이블의 열을 의미.

        == 속성, 특징, 입력, 예측변수, 변수

    - 결과 outcome : 실험이나 연구에서 나오는 결과를 의미

        == 종속변수, 응답, 목표, 출력

    - 레코드 record : 테이블의 각 행

        == 기록값, 사건, 사례, 예제, 패턴, 샘플

1.2.1. 데이터 프레임과 인덱스

: 데이터 베이스에서는 하나의 열을 인덱스로 지정하여 쿼리 성능을 향상 시킨다.

1.2.2. 테이블 형식이 아닌 데이터 구조

- 예1 : 시계열 데이터는 동일한 변수안에 연속적인 측정 값을 가진다.
- 예2. : 공간데이터
    - 객체 : 하나의 실체로 인식될수 있는 것들 ( 건물, 도로 , 필지 등 ) 과 그것의 위치 정보
    - 필드 :일정한 공간상에 연속적으로 분포된 특정 계량값 ( 기온, 수온, 압력 등 ) 을 의미
- 예3 : 그래프 혹은 네트워크 데이터

## 1.3. 위치 추정

: **데이터를 살표보는 가장 기초적인 단계는 각 피처(변수)의 ‘대푯값’을 구하는 것이다.**

이는 곧 **대부분의 값이 어디쯤 위치 하는지 (중심경향성)을 나타내는 추정값**이다.

1. 평균 ( mean, average )

    : 대푯값을 구하는 것으로 대표적으로 생각할 수 있는 방법이다. 평균은 모든 값의 총합을 값의 계수로 나눈 값으로 계산이 용의하고 사용하기도 편하다. 그러나 데이터를 대표로 하는 값으로 적당하지 않다. 평균을 의미하는 기호로 $\bar{x}$ ( '엑스바' ) 라고 읽는다. 

    ```bash
    state['Population'].mean()
    ```

2. 절사 평균 ( trimmed mean )

    : 값들을 크기 순으로 정렬하여, 정해진 개수의 극단값을 제외한 나머지 값들의 평균이다. 이러한 방식을 통해 평균에 비해 극단값의 영향을 덜 받는다. 

    ```python
    from scipy.stats import trim_mean

    trim_mean(state['Population'], 0.1)
    ```

3. 가중 평균 ( weighted mean )

    : 각 데이터 값에  사용자가 지정한 가중치를 곱한 값의 총합(wx)을 가중치의 총합(w)으로 나눈값이다. 이러한 평균을 사용하는 이유는 데이터를 수집할때, 서로 다른 대조군의 수가 다르기 때문에 이를 보정하는 방법으로 부족한 소수 그룹에 더 높은 가중치를 주는 것이다. 

    ```python
    np.average(state['Murder.Rate'], weights=state['Population'])
    ```

4. 중간값 ( median )

    : 데이터를 일렬로 정렬했을때, 한 가운데 위치하는 값이다. **이는 평균에 비해 극단값의 영향을 덜 받게 된다.**

    ```python
    state['Population'].median()
    ```

5. 가중 중간값 (weighted median )

    : 각 데이터는 각자의 가중치를 가지고 있고, 가중 중간값은 순서를 나열하여 가운데 있는 값이 아닌, 어떤 위치를 기준으로 상위 절반의 가중치의 합이 하위 가중치의 합과 동일한 위치의 값이 된다. 

    ```python
    import wquantiles

    wquantiles.median(state['Murder.Rate'], weights=state['Population'])
    ```

결과를 잘못 이해하게 하는 특잇값(극단값)들의 영향을 덜 받은 것을 '로버스트하다' 라고 한다. 이 말은, 모델이나 데이터가 견고하다 라도 생각하면 된다. 즉, 조금더 로버스트한 대표값을 얻기 위해서 평균보다 중간값 혹은 절사 평균 등이 도움이 된다. 

## 1.4. 변이 추정

**: 변이 ( variability ) 는 데이터의 값이 얼마나 밀집 혹은 퍼져있는지를 나타내는 산포도이다.**

**: 중심경향도만으로는 집단에 대한 성격과 분포를 파악하는데 부족하므로, 측정된 데이터가 어떻게 분포하고 있는지에 대해 파악해야 데이터를 제대로 이해할 수 있다.**

: 변이 추정은 **편차**를 기본으로 하는데, 편차란, 관측 데이터와 위치 추정값 사이의 차이이며, 이는 데이터가 중앙값을 주변으로 얼마나 퍼져있는가를 나타낸다.

**1. 평균 절대 편차**

: 편차의 절댓값의 평균을 구하는 것이다. 그냥 단순히 편차 자체를 평균으로 나타내는 것은 옳지 않다. 그 이유는, 음의 편차는 양의 편차를 상쇄해버리기 때문이다. 따라서, 평균 절대 편차라는 방법을 사용하는 것이다.

$$ \frac{\sum_{i=1}^{n} \left | x_{i} - \bar{x} \right|}{n}$$

**2. 분산과 표준 편차**

: 분산은 제곱편차의 평균이고, 표준편차는 분산의 제곱근이다.  표준편차는 분산에서 제곱한 것을 다시 원래의 데이터의 척도(scale)로 돌아가게 해주기 때문에, 해석하기 쉽다.

$$분산 = \frac{\sum_{i=1}^{n} \left (  x_{i} - \bar{x}  \right )^{2}}{n-1} $$

![https://blog.kakaocdn.net/dn/AbXmU/btqMGeSAB8B/smYy1asxp3d31ssXKFuuoK/img.png](https://blog.kakaocdn.net/dn/AbXmU/btqMGeSAB8B/smYy1asxp3d31ssXKFuuoK/img.png)

왼 ) 분산이 큼  오 ) 분산이 작음

- 분산이 큼 : 값들이 퍼져있음. 평균이 값들의 특성을 잘 나타낸다고 말하기 어려움
- 분산이 작음 : 값들이 모여져있음. 값들이 평균값에 모아져 있음.

```python
state['Population'].var() # 분산
state['Population'].std() # 표준 편차
```

**3. 중위 절대 편차 ( MAD : Median Absolute Deviation )**

: 분간, 표준편차, 평균절대편차 모두 로버스트하지 않고 특잇값에 민간하기 때문에 이것을 사용한다.

이것은 관측값에서 중앙값을 빼고, 그 차이에 절댓값을 취한 값들의 중앙값을 구하는 것이다.

```python
from statsmodels import robust

robust.scale.mad(state['Population'])
```

> 통계학에서는 표준편차를 평균절대편차 보다 더 선호하는데, 그 이유는 수학적으로 제곱한 값이 절댓값보다 통계모델을 다룬데 더 편리하다는 통계 이론이 뒷받침하고 있어서 라고 한다.또한, 표준편차는 항상 평균절대편차보다 크다. 그리고 평균 절대편차는 중위절대편차보다 크다.

- 자유도

: 모집단을 기준으로 하지 않고, 표본을 선정해서 표본의 개수 (n-1)로 계산한 분산을 표본분산이라 한다.

** 자유도는 왜 n-1 이냐? : ex, A, B, C 의 값의 평균이 10이라면, A/B는 10 이상/이하이어도 상관 X, 하지만, A/B값이 정해져버리면, C의 값은 무조건 하나의 값으로 정해져야 평균이 10이 됨. 따라서, 자유롭게 값이 변할수 있는 것은 n-1개가 됨.

**4. 사분위 범위 ( IQR )**

: 순서통계량 ( 순위데이터를 나타내는 통계량 ) 에서 가장 큰값과 작은 값의 차이를 나타내는 범위가 가장 기본이 된느 측도이나, 이는 특잇값에 민감하다. 이를 해결하기 위해서 백분위수 사이의 차이를 가지고 식을 추정하는 방법이있다. 그중, 사분위 범위는 25번째 백분위수와 75번재 백분위 수의 차이를 보는 것이다.  데이터가 크면, 데이터를 정렬하는데 많은 연산이 필요로 하게 된다.

- 보통 백분위수는 가중평균이다. R 에서는 분위수 계산하는 방식이 9개가 있지만, 파이썬 numpy.quantile 에는 선형보간법이라는 방법만 지원한다.

```python
state['Population'].quantile(0.75) - state['Population'].quantile(0.25)
```


## 1.5. 데이터 분포 탐색

: 데이터가 전반적으로 어떻게 분포하고 있는지 알아보는 것

1. **백분위수와 상자 그림**
- 백분위수

```python
# quantile 에 해당 백분위수를 입력하면, 그 백분위수의 값을 보여준다.

state['Murder.Rate'].quantile([0.05, 0.25, 0.5, 0.75, 0.95])

# 0.05    1.600  # 5% 백분위 수는 1.6
# 0.25    2.425
# 0.50    4.000
# 0.75    5.550
# 0.95    6.510
```

- 상자그림

: 백분위수를 이용해 데이터의 분산을 손쉽게 시각화 하는 방법 

상자 위쪽과 아래쪽은 각각 75%, 27% 백분위수를 의미하며, 초록색 선(상자안의 선)은 중간값을 의미한다. 

상자 밖으로 뻣어있는 실선은 '수염'으로 데이터의 전체 범위를 나타내 주는 위아래 선과 연결되어있다. 이선은 보통 사분위범위의 1.5배이상 나가지 않도록 구현되어있다. ( Matplotlib )

그 외의 동그란 점은 특잇값(이상치)를 의미한다.

```python
ax = (state['Population']/1000000).plot.box()
ax.set_ylabel('Popluation (millions)')
```

1. **도수분포표와 히스토그램**

도수분포표 : 변수의 범위를 동일한 크기의 구간으로 나눈 다음, 각 구간마다 몇개의 변수값이 존재하는지 보여주기 위해 사용하는 것

: 구간이 너무 크거나 작아도 중요한 특징을 놓치거나 큰그림을 볼수 없다. 따라서 적절한 구간 설정이 중요하다. 

```python
#  pd.cut 함수는 값들을 각 구간에 매핑하는 시리즈를 만드는 것 
population = pd.cut(state.Population, 10) # 균일한 10개의 구간으로 나누기
population.value_counts()
```


도수분포표와 백분위수 모두 구간을 나눠서 데이터를 살표보는 방법이나, 차이점이 있다.
백분위수는 각 구간에 서로 크기가 다르게 구간이 나뉘지만, 도수분포표는 구간의 크기가 같다. 즉 구간별 개수 데이터가 다르다

- 히스토그램 : 도수분포표를 시각화하는 방법

: 히스토그램 역시, 구간은 동일한 크기를 가짐을 확인할 수 있다.

```python
ax = (state['Population'] / 1000000).plot.hist(figsize=(4,4), color='lightcoral')
ax.set_xlabel('Polulation (millions)')
```

통계학에선, 위치와 변이는 분포의 일차 및 이차 모멘트(적률)이라고 하며 삼차,사차 모멘트는 왜도, 첨도라고 부른다. 
왜도는 데이터가  큰값이나 작은값으로 얼마나 비스듬히 쏠려있는지를 나타내고, 첨도는 데이터 극단 값을 갖는 경향성을 나타낸다. 즉 값이 얼마나 뾰족한가 둥그란가 그런 의미이다. 
이것을 확인하는데는 보통 전체적인 시각화를 통해 확인한다.

1. 밀도 그림 추정

: 밀도 그림은 데이터의 분포를 연속된 선으로 보여준다.

y축이 개수가 아닌 비율을 표시한다는 점에서 히스토그램과 차이를 보인다. 밀도 곡선 아래의 총면적은 1이고 구간의 개수대신 x축의 두점사이의 곡선 아래 면적을 계산한다. 

```python
ax = state['Murder.Rate'].plot.hist(density=True, xlim = [0,12], bins = range(1,12) , color='moccasin')
state['Murder.Rate'].plot.density(ax=ax, color='sienna') # 밀도 그림을 생성하기 위한 메서드 
ax.set_xlabel('Murder Rate ( per 100,000)')
```

## 1.6. 이진 데이터와 범주데이터 탐색

위에서 정리한 것은 연속형 데이터에 대한 부분이었다. 다음 아래는 이진 데이터와 범주 데이터를 탐색할 때의 방식을 정리한 것이다.

1. 막대 도표

: 범주형 자료를 보여줄 때 주로 사용되며, 기본적인 시각화 방법이다. 

```python
ax = dfw.transpose().plot.bar(figsize = (4,4), legend =False, color='darkkhaki')
ax.set_xlabel('Cause of delay')
ax.set_ylabel('Count')
```

히스토그램은 수치적으로 나타낼수있는 변수를 그리고, 막대도표는 각 요인 변수를 그린다. 

1. 최빈값 

: 데이터에서 가장 자주 등장하는 값을 의미한다.

1. 기댓값 

: 범주형 데이터들 중에, 각 범주에 해당하는 수치형 변수들이 존재한다. 이 변수들은 범주형이라는 말처럼 이어지는 것이아니라 몇가지로 고정되어있다. 

1. 확률

: 사건이 수없이 반복될 경우 사건이 발생할 비율 

## 1.7. 상관관계

- 양의 상관관계 : X 가 큰값이면 Y 도 큰값을 가지는 것 ( 반대의 경우도 해당 )
- 음의 상관관계 : X가 큰값을 가지는데 Y는 작은 값을 가지는 것 ( 반대의 경우도 해당 )

1. 상관계수 ( 피어슨 상관계수 ) : 두 변수 사이의 상관관계를 항상 같은 척도에 넣고 추정하는 것 

: 상관계수는 -1 ~ 1까지의 표준화된 측정 지표이다.  0은 상관성이 없음을 나타낸다. 

: 상관계수는 데이터의 특잇값에 민감하다. 그래서 로버스트한 방법을 제공하는 `sklearn.covariance` 패키지에 다양한 방법이 있다.  상관계수는 선형일때만 관계를 가지고 있다. 

$${\displaystyle r_{XY}={{{\sum {i}^{n}\left(X{i}-{\overline {X}}\right)\left(Y_{i}-{\overline {Y}}\right)} \over {n-1}} \over {{\sqrt {{\sum {i}^{n}\left(X{i}-{\overline {X}}\right)^{2}} \over {n-1}}}{\sqrt {{\sum {i}^{n}\left(Y{i}-{\overline {Y}}\right)^{2}} \over {n-1}}}}}}$$

```python
import seaborn as sns 

etfs.corr()  # 상관관계를 구하기

etfs = sp500_px.loc[sp500_px['Unnamed: 0'] > '2012-07-01', sp500_sym[sp500_sym['sector'] == 'etf']['symbol']]
sns.heatmap(etfs.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20,220, as_cmap=True)) # heatmap 으로 상관관계를 시각화 ( sns.heatmap() )
```

1. 산점도

: 두 변수 사이의 관계를 시각화 하는 기본적인 방법 

```python
etfs.plot.scatter(x = 'SPY', y = 'DIA', figsize=(4,4), marker = '$\u25EF$')
```

## 1.8. 두개 이상의 변수 탐색하기

평균과 분산과 같은 추정값들은 한번에 하나의 변수를 다룬다. ( 일변량 분석 ) . 상관분석은 두변수를 비교할때 사용된다. ( 이변량분석 )

1. 육각형 구간 ( 수치형 변수 대 수치형 변수를 시각화 )

: 점으로 표시하는 대신 기록값을 육각형의 모양의 구간들로 나누고 기록값의 개수에 따라 색깔을 표시한다. 

```python
ax = kc_tax0.plot.hexbin(x = 'SqFtTotLiving', y = 'TaxAssessedValue',
                        gridsize = 30, sharex=False, figsize = (5,4))
```

1. 등고선  ( 수치형 변수 대 수치형 변수를 시각화 )

: 두 변수로 이루어진 지형에서의 등고선. 등고선 위의 점들은 밀도와 같다. 꼭대기 쪽으로 갈수록 밀도는 높아진다. 

```python
fig, ax = plt.subplots(figsize=(4,4))
ax = sns.kdeplot(data=kc_tax0, x = 'SqFtTotLiving', y = 'TaxAssessedValue')
```

1. 히트맵 ( 수치형 변수 대 수치형 변수를 시각화 )

: 두 수치형 변수의 관계를 나타내는 또 다른 방법

히트맵, 육각구간, 등고도표 모두 이차원상의 밀도를 시각화 하는데 사용된다. 

1. 범주형 변수 대 범주형 변수
- 분할표

    : 두 범주형 변수를 요약하는데 효과적인 방법으로 범주별 빈도수를 기록한 표이다. 

파이썬은 pivot_table을 통해 피벗 테이블을 만든다.

```python
# magins 키워드 인수는 열과 행의 함계를 추가할수있다.
crosstab = lc_loans.pivot_table(index='grade', columns='status', aggfunc=lambda x: len(x), margins=True)

# 열합계를 무시하고 피벗테이블의 복사본을 만든다
df= crosstab.loc['A':'G',:].copy()

# 행 합계로 행을 나눈다.
df.loc[:, 'Charged Off':'Late'] = df.loc[:, 'Charged Off':'Late'].div(df['All'], axis=0)

# 'All' 열을 총합으로 나눈다.
df['All'] = df['All'] / sum(df['All'])
perc_crosstab = df
```
1. 범주형 변수 대 수치형 변수
- 상자 그림

    : 범주형 변수에 따라 분류된 수치형 변수의 분포를 시각화하여 비교하는 간단한 방법이다.

    데이터의 특잇값을 좀더 명확하게 보여준다.
```python
ax = airline_stats.boxplot(by='airline', column='pct_carrier_delay')
ax.set_xlabel('')
ax.set_ylabel('Daily % of Dalayed Flights')
plt.suptitle('')
```
  
- 바이올린 도표

    : 상자 그림을 보완한 형태로, y축을 따라 밀도 추정 결과를 동시에 시각화 한것. 

    위 그림과 비교해서 보면 알레스카와 델타의 데이터가 0에 집준됨을 확인할 수있다. 

    geom_boxplot 을 사용하면 바이올린 도표에 상자그림을 결합할 수 있다. 
```python
ax = sns.violinplot(airline_stats.airline, airline_stats.pct_carrier_delay,
                   inner='quartile', color='white')
ax.set_xlabel('')
ax.set_ylabel('Daily % of Delayed Flights')
```
   
1. 다변수 시각화 하기.

조건화 라는 개념을 통해 두 변수 비교용 도표를 더 여러 변수를 비교하는 용도로 확장하여 활용할 수있다. 

```python
zip_codes = [98188, 98105, 98108, 98126]
kc_tax_zip = kc_tax0.loc[kc_tax0.ZipCode.isin(zip_codes),:]
kc_tax_zip

def hexbin(x, y, color, **kwargs):
    cmap = sns.light_palette(color, as_cmap=True)
    plt.hexbin(x, y, gridsize=25, cmap=cmap, **kwargs)

    
# 조건 변수를 지정하려면 col, row 인수를 사용한다. 
# 단일 조건 변수의 경우 col_wrap 과 함께 col을 사용하여 패싯 그래프를 여러행으로 래핑한다.
g = sns.FacetGrid(kc_tax_zip, col='ZipCode', col_wrap=2)

# map 메서드는 다른 우편번호에 대한 원래 데이터 집합의 서브셋에 대해 hexbin 함수를 호출한다.
# extent는 x 측과 y축의 한계를 정의한다.
g.map(hexbin, 'SqFtTotLiving', 'TaxAssessedValue', 
      extent=[0, 3500, 0, 700000])
g.set_axis_labels('Finished Square Feet', 'Tax Assessed Value')
g.set_titles('Zip code {col_name:.0f}')
```